# English_French_LT_models
This repo contains various imodel mplementations of the Language Transaltion Task, ranging from Transformer to Encoder - Decoder models 

The Main Goal of this repo is it highlights the evolution of algorithms for NMT task.

The Repo is loadded with files that can handle any amount data with little to no changes.

I have used customised Vectorization of text for the task, though I highly recommend use of 

https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization

The Models in the Repo were inspired by following research papers:- 

Attention is all you need --> https://arxiv.org/abs/1706.03762

Neural Machine Translation by Jointly Learning to Align and Translate --> https://arxiv.org/abs/1409.0473

Sequence to Sequence Learning with Neural Networks --> https://arxiv.org/abs/1409.3215

Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation --> 

https://arxiv.org/abs/1406.1078


## Resources that might help in better understanding the papers linked above

Credit to the Authors for amazing explaination on the topics

Transformer Blog post --> http://jalammar.github.io/illustrated-transformer/

Coursera Specialisation --> https://www.coursera.org/specializations/natural-language-processing?

Blog post on Transformer --> https://towardsdatascience.com/transformers-141e32e69591

YouTube Videos by Rasa on Attention --> https://www.youtube.com/watch?v=tIvKXrEDMhk
